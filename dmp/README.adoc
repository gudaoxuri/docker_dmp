== DMP Environment

*Based tairangroup/kylin:latest , Depend tairangroup/spark:latest tairangroup/confluent:latest tairangroup/hive:latest tarrangroup/hbase:latest*

=== Features

* SSH server & client ( Port: 22 User: root Password: 123456 )
* Hadoop 2.7.x
* Confluent 4.x
* Hive 2.3.x
* HBase 1.3.x
* Phoenix 4.13.x
* Spark 2.2.x
* Kylin 2.2.x
* Livy 0.5.x

=== Examples

Start ZK:

 docker run --name dmp_zookeeper -p 2181:2181 -d zookeeper

Start MySQL :

 docker run --name dmp_mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql

Start Redis:

 docker run --name dmp_redis -p 6379:6379 -d redis

Start ES With SQL:

 docker run --name dmp_es -p 9200:9200 -e "ES_JAVA_OPTS=-Xms256m -Xmx256m" -d elasticsearch
 docker exec -it dmp_es bash
 > /bin/elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/5.6.7.0/elasticsearch-sql-5.6.7.0.zip
 docker restart dmp_es

Start DMP :

 docker run --name dmp -h dmp \
    -P -p 4000-4010:4000-4010 \
    -p 222:22 \
    -p 9092:9092 \
    -p 10000:10000 \
    -p 8765:8765 \
    -p 8088:8088 \
    -p 7070:7070 \
    -p 8998:8998 \
    -p 4040:4040 -p 7077:7077 -p 18080:18080 -p 18081:18081 \
    -p 9000:9000 -p 50010:50010 -p 50020:50020 -p 50070:50070 -p 50075:50075 \
    --link dmp_mysql:mysql --link dmp_zookeeper:zookeeper --link dmp_redis:redis \
    -e KEEP=true -e KAFKA_ADVERTISED_HOST_NAME=10.200.131.25 \
    -v /data/docker_data/dmp/workspaces:/opt/workspaces \
    -d tairangroup/dmp

Operation：

[source,shell]
----
Hadoop Home:/opt/

example:

# HDFS
hdfs dfs -mkdir /test
hdfs dfs -ls /

# Hive
> cat>test_person.txt<<EOF
tom 20
jack    24
nestor  29
EOF
> hdfs dfs -mkdir /tmp/hivetest/
> hdfs dfs -put ./test_person.txt /tmp/hivetest/
> hive
CREATE EXTERNAL TABLE test_person(name STRING,age INT)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LOCATION '/tmp/hivetest';
select * from test_person;

> beeline -u jdbc:hive2://localhost:10000
select * from test_person;

# HBase
> hbase shell
create 'test', 'cf'
list 'test'
put 'test', 'row1', 'cf:a', 'value1'
put 'test', 'row2', 'cf:b', 'value2'
put 'test', 'row3', 'cf:c', 'value3'
scan 'test'
get 'test', 'row1'
disable 'test'
drop 'test'

> /opt/phoenix/bin/sqlline.py zookeeper:2181
!tables
> /opt/phoenix/bin/sqlline.py zookeeper:2181 /opt/phoenix/examples/STOCK_SYMBOL.sql
select * from stock_symbol;

# Spark
> run-example org.apache.spark.examples.SparkPi

> spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://dmp:7077\
  --deploy-mode cluster \
  --driver-memory 512M \
  --executor-memory 512M \
  /opt/spark/examples/jars/spark-examples_2.11-2.2.1.jar \
  1000

# Confluent
> kafka-avro-console-producer \
         --broker-list localhost:9092 --topic test \
         --property value.schema='{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'
> {"f1": "value1"}
> {"f1": "value2"}
> {"f1": "value3"}

> kafka-avro-console-consumer --topic test \
         --bootstrap-server localhost:9092 \
         --from-beginning

# Kylin
打开浏览器，输入http://localhost:7070
账密：ADMIN:KYLIN
1. 选择project：learn-kylin
2. 选择cube：   kylin_sales_cube  点击 action -> build 进行构建cube -> 进入Monitor界面查看状态
3. 执行sql查询： build成功后，-> Insight，输入sql语句
    得到fact table所缓存的列——均为dimension的主key、measure中所需计算的字段。
    -> select * from kylin_sales
    各个时间段内的销售额及购买量：
    -> select part_dt, sum(price) as total_selled, count(distinct seller_id) as sellers
       from kylin_sales
       group by part_dt
       order by part_dt
# Kylin stream
1. 往队列  kylin_streaming_topic 添加数据
> kafka-console-producer \
         --broker-list localhost:9092 --topic kylin_streaming_topic \
> {"country":"CANADA","amount":28.410708132590624,"qty":6,"currency":"USD","order_time":1518054870159,"category":"Other","device":"Andriod","user":{"gender":"Female","id":"696ff071-20fa-444c-a029-07f92f06cabc","age":27}}
> {"country":"CHINA","amount":59.28096379181497,"qty":6,"currency":"USD","order_time":1518054870169,"category":"ELECTRONIC","device":"Other","user":{"gender":"Male","id":"1e3a9bc1-e4ac-4869-810a-e7d01381b7c8","age":26}}

2. 选择cube：   kylin_streaming_cube  点击 action -> build 进行构建cube -> 进入Monitor界面查看状态
3. 执行sql查询： build成功后，-> Insight，输入sql语句

   select minute_start, count(*), sum(amount), sum(qty) from streaming_sales_table group by minute_start order by minute_start

# Livy
usage batch session :
1. Set up the Headers : key -> Content-Type, value -> application/json
2. Create batch session : POST/
   > Set up the URL : http://10.200.131.25:8998/batches
   > Set up the Body : such as :
   {
   	"name": "storage-rds",
   	"file": "/storage/rds/storage-rds-1.0.0-SNAPSHOT-jar-with-dependencies.jar",
   	"className": "com.tairanchina.csp.dmp.components.storage.rds.RDSMain",
   	"args": ["-k","10.200.131.25:9092",
   		"-t","dmp.source.api-visit",
   		 "-c","visit"
   	],
   	"driverMemory": "512M",
   	"driverCores": 2,
   	"executorMemory": "512M",
   	"executorCores": 2,
   	"numExecutors": 10
   }
3. Check the status : GET/
   > Set up the URL : http://10.200.131.25:8998/batches/{batchId}/state
4. Delete the batch session : DELETE/
   > Set up the URL : http://10.200.131.25:8998/batches/{batchId}



----

=== Environments

|===
| Env | Default Value | Remark

| TZ | Asia/Shanghai |
| KEEP | false | true = always run
| KAFKA_ADVERTISED_HOST_NAME | HOSTNAME |
| KAFKA_ADVERTISED_PORT | 9092 |
| KAFKA_PORT | 9092 |
|===

=== Volumes

|===
| volume | Remark

| /data/hadoop/hdfs/nn | Name node path
| /data/hadoop/hdfs/dn | Data node path
| /opt/confluent/share/java | jars
|===

=== Expose Ports

|===
| Port | Remark

| 22 | SSH Port

| 9092 | Kafka Service Port

| 10000 | Service for programatically (Thrift/JDBC) connecting to Hive,ENV Variable HIVE_PORT

| 60000 | ``hbase.master.port``
| 60010 | The port for the HBase­Master web UI. Set to -1 if you do not want the info server to run. ``hbase.master.info.port``
| 60030 | ``hbase.regionserver.info.port``
| 8765 | HBase Query Server default port

| 4040 | ``REST API``
| 7077 | ``SPARK_MASTER_PORT``
| 18080 | ``SPARK_MASTER_WEBUI_PORT``
| 18081 | ``SPARK_WORKER_WEBUI_PORT``

| 8998 | ``LIVY_REST_PORT``

| 9000 | File system metadata operations ``fs.default.name``
| 50010 | Data transfer ``dfs.datanode.address``
| 50020 | Metadata operations ``dfs.datanode.ipc.address``
| 50070 | Web UI to look at current status of HDFS, explore file system ``dfs.http.address``
| 50075 | DataNode WebUI to access the status, logs etc. ``dfs.datanode.http.address``
| 50090 | Checkpoint for NameNode metadata ``dfs.secondary.http.address``
|===